{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification using Distributed Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 1\n",
      "Number of desired processes / executor: 1\n",
      "Total number of workers: 1\n"
     ]
    }
   ],
   "source": [
    "from distkeras.evaluators as dke \n",
    "from distkeras.predictors as dkp\n",
    "import distkeras.trainers as dktr\n",
    "import distkeras.transformers as dktf\n",
    "from distkeras.utils import *\n",
    "\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.core import *\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import *\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "if os.name == 'nt':\n",
    "    import distkeras.pwd as pwd\n",
    "else:\n",
    "    import pwd\n",
    "\n",
    "# First, setup the Spark variables. You can modify them to your needs.\n",
    "application_name = \"Distributed Keras MNIST Notebook\"\n",
    "\n",
    "local = True\n",
    "path_train = \"data/mnist_train.csv\"\n",
    "path_test = \"data/mnist_test.csv\"\n",
    "if local:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_processes = 2\n",
    "    num_executors = 8\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    num_executors = 20\n",
    "    num_processes = 1\n",
    "\n",
    "# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_processes\n",
    "\n",
    "print(\"Number of desired executors: {}\".format(num_executors))\n",
    "print(\"Number of desired processes / executor: {}\".format(num_processes))\n",
    "print(\"Total number of workers: {}\".format(num_workers))\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", num_processes)\n",
    "conf.set(\"spark.executor.instances\", num_executors)\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "conf.set(\"spark.local.dir\", \"/tmp/\" + get_os_username() + \"/dist-keras\");\n",
    "\n",
    "\n",
    "sparkSession = SparkSession.builder.config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the training dataset.\n",
    "raw_dataset_train = (sparkSession\n",
    "                     .read.csv(path_train, header='true', inferSchema='true')\n",
    "                     )\n",
    "\n",
    "# Read the testing dataset.\n",
    "raw_dataset_test = (sparkSession\n",
    "                     .read.csv(path_test, header='true', inferSchema='true')\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      5       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_train.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 225)               1037025   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 225)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2260      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,048,853\n",
      "Trainable params: 1,048,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "# This is identical for the test set.\n",
    "features = raw_dataset_train.columns\n",
    "features.remove('label')\n",
    "\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column\n",
    "# \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset_train = vector_assembler.transform(raw_dataset_train)\n",
    "dataset_test = vector_assembler.transform(raw_dataset_test)\n",
    "\n",
    "# Define the number of output classes.\n",
    "nb_classes = 10\n",
    "encoder = dktf.OneHotTransformer(nb_classes, input_col=\"label\", output_col=\"label_encoded\")\n",
    "dataset_train = encoder.transform(dataset_train)\n",
    "dataset_test = encoder.transform(dataset_test)\n",
    "\n",
    "# Allocate a MinMaxTransformer from Distributed Keras to normalize the features..\n",
    "# o_min -> original_minimum\n",
    "# n_min -> new_minimum\n",
    "transformer = dktf.MinMaxTransformer(n_min=0.0, n_max=1.0, \\\n",
    "                                     o_min=0.0, o_max=250.0, \\\n",
    "                                     input_col=\"features\", \\\n",
    "                                     output_col=\"features_normalized\")\n",
    "# Transform the dataset.\n",
    "dataset_train = transformer.transform(dataset_train)\n",
    "dataset_test = transformer.transform(dataset_test)\n",
    "\n",
    "# Keras expects the vectors to be in a particular shape, we can reshape the\n",
    "# vectors using Spark.\n",
    "reshape_transformer = dktf.ReshapeTransformer(\"features_normalized\", \"matrix\", (28, 28, 1))\n",
    "dataset_train = reshape_transformer.transform(dataset_train)\n",
    "dataset_test = reshape_transformer.transform(dataset_test)\n",
    "\n",
    "# Now, create a Keras model.\n",
    "# Taken from Keras MNIST example.\n",
    "\n",
    "# Declare model parameters.\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Construct the model.\n",
    "convnet = Sequential()\n",
    "convnet.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                          border_mode='valid',\n",
    "                          input_shape=input_shape))\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(MaxPooling2D(pool_size=pool_size))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(225))\n",
    "convnet.add(Activation('relu'))\n",
    "convnet.add(Dense(nb_classes))\n",
    "convnet.add(Activation('softmax'))\n",
    "\n",
    "# Define the optimizer and the loss.\n",
    "optimizer_convnet = 'adam'\n",
    "loss_convnet = 'categorical_crossentropy'\n",
    "\n",
    "# Print the summary.\n",
    "convnet.summary()\n",
    "\n",
    "# We can also evaluate the dataset in a distributed manner.\n",
    "# However, for this we need to specify a procedure how to do this.\n",
    "def evaluate_accuracy(model, test_set, features=\"matrix\"):\n",
    "    evaluator = dke.AccuracyEvaluator(prediction_col=\"prediction_index\", label_col=\"label\")\n",
    "    predictor = dkp.ModelPredictor(keras_model=model, features_col=features)\n",
    "    transformer = dktf.LabelIndexTransformer(output_dim=nb_classes)\n",
    "    test_set = test_set.select(features, \"label\")\n",
    "    test_set = predictor.predict(test_set)\n",
    "    test_set = transformer.transform(test_set)\n",
    "    score = evaluator.evaluate(test_set)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Select the desired columns, this will reduce network usage.\n",
    "dataset_train = dataset_train.select(\"features_normalized\", \"matrix\",\"label\", \"label_encoded\")\n",
    "dataset_test = dataset_test.select(\"features_normalized\", \"matrix\",\"label\", \"label_encoded\")\n",
    "# Keras expects DenseVectors.\n",
    "dense_transformer = dktf.DenseTransformer(input_col=\"features_normalized\", output_col=\"features_normalized_dense\")\n",
    "dataset_train = dense_transformer.transform(dataset_train)\n",
    "dataset_test = dense_transformer.transform(dataset_test)\n",
    "dataset_train.repartition(num_workers)\n",
    "dataset_test.repartition(num_workers)\n",
    "# Assing the training and test set.\n",
    "training_set = dataset_train.repartition(num_workers)\n",
    "test_set = dataset_test.repartition(num_workers)\n",
    "# Cache them.\n",
    "training_set.cache()\n",
    "test_set.cache()\n",
    "\n",
    "# Precache the trainingset on the nodes using a simple count.\n",
    "print(training_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 10038] An operation was attempted on something that is not a socket\n",
      "[WinError 10061] No connection could be made because the target machine actively refused it\n",
      "Training time: 1745.5220806598663\n",
      "Accuracy: 0.9883\n",
      "Number of parameter server updates: 3751\n"
     ]
    }
   ],
   "source": [
    "# Use the ADAG optimizer. You can also use a SingleWorker for testing purposes -> traditional\n",
    "# non-distributed gradient descent.\n",
    "trainer = dktr.ADAG(keras_model=convnet, worker_optimizer=optimizer_convnet, loss=loss_convnet,\n",
    "                    num_workers=num_workers, batch_size=16, communication_window=5, num_epoch=5,\n",
    "                    features_col=\"matrix\", label_col=\"label_encoded\")\n",
    "\n",
    "trained_model = trainer.train(training_set)\n",
    "\n",
    "print(\"Training time: \" + str(trainer.get_training_time()))\n",
    "print(\"Accuracy: \" + str(evaluate_accuracy(trained_model, test_set)))\n",
    "print(\"Number of parameter server updates: \" + str(trainer.parameter_server.num_updates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
